{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Custom_VQA1_R.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"FrnHzklAsBmt"},"source":["# INTRO"]},{"cell_type":"code","metadata":{"id":"KJqHEF3eT6Zn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611944193470,"user_tz":-60,"elapsed":33580,"user":{"displayName":"Yeah Go","photoUrl":"","userId":"04798422462734009754"}},"outputId":"ee324c69-fa71-469b-8de2-fd3550ee4352"},"source":["from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","import os\n","import tensorflow as tf\n","import numpy as np\n","import json\n","\n","\n","# Set the seed for random operations. \n","# This let our experiments to be reproducible. \n","SEED = 1234\n","tf.random.set_seed(SEED)\n","np.random.seed(SEED)\n","\n","# Get current working directory\n","cwd = os.getcwd()\n","\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","data_dir=r\"/content/drive/MyDrive/anndl-2020-vqa.zip (Unzipped Files)/VQA_Dataset\"\n","image_dir=r\"/content/drive/MyDrive/anndl-2020-vqa.zip (Unzipped Files)/VQA_Dataset/Images\"\n","\n","os.listdir(data_dir)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['Images', 'test_questions.json', 'train_questions_annotations.json', 'Splits']"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"N1e23RyaQDuU"},"source":["# PREPROCESSING TEXT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKBe8oThPvRe","executionInfo":{"status":"ok","timestamp":1611944216123,"user_tz":-60,"elapsed":2745,"user":{"displayName":"Yeah Go","photoUrl":"","userId":"04798422462734009754"}},"outputId":"4356229c-d5d3-4703-97c5-80b081c4ce32"},"source":["question_sentences = []\r\n","answer = []\r\n","question_sentences_test = []\r\n","\r\n","\r\n","f=open(os.path.join(data_dir, 'train_questions_annotations.json'), encoding='utf-8')\r\n","g=json.load(f)\r\n","count=0\r\n","for line in g:\r\n","  answer.append(g[line]['answer'] ) \r\n","  h=g[line]['question']\r\n","  question_sentences.append( '<sos>' + h.replace('?', ''))\r\n","\r\n","length=len(answer)\r\n","\r\n","type(question_sentences[0])\r\n","print('\\n')\r\n","\r\n","f=open(os.path.join(data_dir, 'test_questions.json'), encoding='utf-8')\r\n","g=json.load(f)\r\n","for line in g:\r\n","  h=g[line]['question']\r\n","  question_sentences_test.append( '<sos>' + h.replace('?', ''))\r\n","\r\n","\r\n","from tensorflow.keras.preprocessing.text import Tokenizer\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","MAX_NUM_WORDS=60000\r\n","\r\n","# Create Tokenizer to convert words to integers\r\n","quest_tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\r\n","quest_tokenizer.fit_on_texts(question_sentences)\r\n","quest_tokenized = quest_tokenizer.texts_to_sequences(question_sentences)\r\n","\r\n","quest_wtoi = quest_tokenizer.word_index\r\n","print('Total question words:', len(quest_wtoi))\r\n","max_quest_length = max(len(sentence) for sentence in quest_tokenized)\r\n","print('Max question sentence length:', max_quest_length)\r\n","\r\n","quest_test_tokenizer = Tokenizer(num_words= MAX_NUM_WORDS)\r\n","quest_test_tokenizer.fit_on_texts(question_sentences_test)\r\n","quest_test_tokenized = quest_tokenizer.texts_to_sequences(question_sentences_test)\r\n","\r\n","quest_test_wtoi = quest_test_tokenizer.word_index\r\n","print('Total question words:', len(quest_test_wtoi))\r\n","max_quest_test_length = max(len(sentence) for sentence in quest_test_tokenized)\r\n","print('Max test question sentence length:', max_quest_test_length)\r\n","\r\n","max_length=max(max_quest_length,max_quest_test_length)\r\n","quest_encoder_inputs = pad_sequences(quest_tokenized, maxlen=max_length)\r\n","type(quest_encoder_inputs)\r\n","\r\n","quest_test_encoder_inputs = pad_sequences(quest_test_tokenized, maxlen=max_length)\r\n","type(quest_test_encoder_inputs)\r\n","\r\n","num_answers=58\r\n","labels_dict =  {\r\n","        '0': 0,\r\n","        '1': 1,\r\n","        '2': 2,\r\n","        '3': 3,\r\n","        '4': 4,\r\n","        '5': 5,\r\n","        'apple': 6,\r\n","        'baseball': 7,\r\n","        'bench': 8,\r\n","        'bike': 9,\r\n","        'bird': 10,\r\n","        'black': 11,\r\n","        'blanket': 12,\r\n","        'blue': 13,\r\n","        'bone': 14,\r\n","        'book': 15,\r\n","        'boy': 16,\r\n","        'brown': 17,\r\n","        'cat': 18,\r\n","        'chair': 19,\r\n","        'couch': 20,\r\n","        'dog': 21,\r\n","        'floor': 22,\r\n","        'food': 23,\r\n","        'football': 24,\r\n","        'girl': 25,\r\n","        'grass': 26,\r\n","        'gray': 27,\r\n","        'green': 28,\r\n","        'left': 29,\r\n","        'log': 30,\r\n","        'man': 31,\r\n","        'monkey bars': 32,\r\n","        'no': 33,\r\n","        'nothing': 34,\r\n","        'orange': 35,\r\n","        'pie': 36,\r\n","        'plant': 37,\r\n","        'playing': 38,\r\n","        'red': 39,\r\n","        'right': 40,\r\n","        'rug': 41,\r\n","        'sandbox': 42,\r\n","        'sitting': 43,\r\n","        'sleeping': 44,\r\n","        'soccer': 45,\r\n","        'squirrel': 46,\r\n","        'standing': 47,\r\n","        'stool': 48,\r\n","        'sunny': 49,\r\n","        'table': 50,\r\n","        'tree': 51,\r\n","        'watermelon': 52,\r\n","        'white': 53,\r\n","        'wine': 54,\r\n","        'woman': 55,\r\n","        'yellow': 56,\r\n","        'yes': 57\r\n","  }\r\n","\r\n","\r\n","ans_indices = [labels_dict[a] for a in answer]\r\n","ans_Y = tf.keras.utils.to_categorical(ans_indices)\r\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"stream","text":["\n","\n","Total question words: 4641\n","Max question sentence length: 22\n","Total question words: 1374\n","Max test question sentence length: 19\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":2},{"output_type":"execute_result","data":{"text/plain":["numpy.ndarray"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"RvnrFaBTWg8q"},"source":["# PREPROCESSING IMAGE\n"]},{"cell_type":"code","metadata":{"id":"LX4woViiVLL1"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n","\r\n","apply_data_augmentation = False\r\n","\r\n","# Create training ImageDataGenerator object\r\n","\r\n","if apply_data_augmentation:\r\n","    img_data_gen = ImageDataGenerator(rotation_range=10,\r\n","                                      width_shift_range=10,\r\n","                                      height_shift_range=10,\r\n","                                      zoom_range=0.3,\r\n","                                      horizontal_flip=True,\r\n","                                      vertical_flip=True,\r\n","                                      fill_mode='reflect')\r\n","else:\r\n","    img_data_gen = ImageDataGenerator(rescale=1./255)\r\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pb908tTuOVZm"},"source":["from PIL import Image\r\n","\r\n","class CustomDataset(tf.keras.utils.Sequence):\r\n","\r\n","  \"\"\"\r\n","    CustomDataset inheriting from tf.keras.utils.Sequence.\r\n","\r\n","    3 main methods:\r\n","      - __init__: save dataset params like directory, filenames..\r\n","      - __len__: return the total number of samples in the dataset\r\n","      - __getitem__: return a sample from the dataset\r\n","\r\n","    Note: \r\n","      - the custom dataset return a single sample from the dataset. Then, we use \r\n","        a tf.data.Dataset object to group samples into batches.\r\n","      - in this case we have a different structure of the dataset in memory. \r\n","        We have all the images in the same folder and the training and validation splits\r\n","        are defined in text files.\r\n","\r\n","  \"\"\"\r\n","\r\n","  def __init__(self, dataset_dir, which_subset, img_generator=None, mask_generator=None, \r\n","               preprocessing_function=None, out_shape=[256, 256]):\r\n","    \r\n","\r\n","    f=open(os.path.join(data_dir, 'train_questions_annotations.json'), encoding='utf-8')   \r\n","    subset_file=json.load(f)\r\n","    \r\n","    subset_filenames = []\r\n","    answer=[]\r\n","\r\n","    for line in subset_file:\r\n","      subset_filenames.append(subset_file[line]['image_id'])\r\n","      \r\n","    \r\n","    \r\n","    self.which_subset = which_subset\r\n","    self.dataset_dir = dataset_dir\r\n","    self.subset_filenames = subset_filenames\r\n","    self.img_generator = img_generator\r\n","    self.preprocessing_function = preprocessing_function\r\n","    self.out_shape = out_shape\r\n","\r\n","  def __len__(self):\r\n","    return len(self.subset_filenames)\r\n","\r\n","  def __getitem__(self, index):\r\n","    # Read Image\r\n","    curr_filename = self.subset_filenames[index]\r\n","    img = Image.open(os.path.join(self.dataset_dir, 'Images', curr_filename + '.png'))\r\n","    \r\n","\r\n","    # Resize image and mask\r\n","    img = img.resize([256,256]) ##MANUALE\r\n","    \r\n","    img_arr = np.array(img)[...,:3]\r\n","    arr=img_arr\r\n","    img_arr = ((arr - arr.min()) * (1/(arr.max() - arr.min()) * 255)).astype('uint8')\r\n","\r\n","\r\n","\r\n","    if self.which_subset == 'training':\r\n","      if self.img_generator is not None:\r\n","        # Perform data augmentation\r\n","        # We can get a random transformation from the ImageDataGenerator using get_random_transform\r\n","        # and we can apply it to the image using apply_transform\r\n","        img_t = self.img_generator.get_random_transform(img_arr.shape, seed=SEED)\r\n","        img_arr = self.img_generator.apply_transform(img_arr, img_t)\r\n","        # ImageDataGenerator use bilinear interpolation for augmenting the images.\r\n","        \r\n","    \r\n","      \r\n","    \r\n","    if self.preprocessing_function is not None:\r\n","      img_arr = self.preprocessing_function(img_arr)\r\n","    \r\n","    q=quest_encoder_inputs[index] ##extract from text preprocessing\r\n","  \r\n","    a=ans_Y[index]\r\n","\r\n","    #inputs=(img_arr,q)\r\n","    inputs={'input_1': img_arr,'input_2':q}\r\n","\r\n","    output=a\r\n","\r\n","    return inputs,output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LF5hkwhsOn-Z"},"source":["from tensorflow.keras.applications.vgg16 import preprocess_input \r\n","\r\n","img_h = 256\r\n","img_w = 256\r\n","\r\n","dataset = CustomDataset(data_dir, 'training', \r\n","                        img_generator=img_data_gen,  #else=img_data_gen\r\n","                        preprocessing_function=preprocess_input)\r\n","\r\n","\r\n","\r\n","types = ( (tf.float32,tf.int64), (tf.int64) ) \r\n","shapes = (([img_h, img_w, 3],[max_quest_length]),\r\n","          [58])\r\n","\r\n","types2= ( { 'input_1':tf.float32, 'input_2' : tf.int64}, tf.int64) \r\n","shapes2 = ({ 'input_1':[img_h, img_w, 3], 'input_2' : [max_quest_length]},\r\n","          [58])\r\n","\r\n","full_dataset = tf.data.Dataset.from_generator(lambda: dataset,\r\n","                                               output_types=types2,\r\n","                                               output_shapes=shapes2)\r\n","\r\n","bs=64\r\n","train_size = int(0.8 * length)\r\n","valid_size=length-train_size\r\n","\r\n","#full_dataset = full_dataset.shuffle()\r\n","train_dataset = full_dataset.take(train_size)\r\n","valid_dataset = full_dataset.skip(train_size)\r\n","\r\n","\r\n","train_dataset = train_dataset.batch(bs)\r\n","train_dataset = train_dataset.repeat()\r\n","valid_dataset = valid_dataset.batch(bs)\r\n","valid_dataset = valid_dataset.repeat()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tn1uuNPvb9Co"},"source":["# Let's test data generator\r\n","# -------------------------\r\n","import time\r\n","from matplotlib import cm\r\n","import matplotlib.pyplot as plt\r\n","\r\n","%matplotlib inline\r\n","\r\n","\r\n","train_dataset\r\n","next(iter(train_dataset))[0]['input_1']\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JSR_e9lfPzct"},"source":["# MODEL"]},{"cell_type":"code","metadata":{"id":"UpD3FPjtK-fw"},"source":["# Import Keras \n","from keras.layers import Conv2D, MaxPooling2D, Flatten\n","from keras.layers import Input, LSTM, Embedding, Dense\n","from keras.models import Model, Sequential\n","\n","# Define CNN for Image Input\n","vision_model = Sequential()\n","vision_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(256, 256, 3)))\n","vision_model.add(Conv2D(64, (3, 3), activation='relu'))\n","vision_model.add(MaxPooling2D((2, 2)))\n","vision_model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n","vision_model.add(Conv2D(128, (3, 3), activation='relu'))\n","vision_model.add(MaxPooling2D((2, 2)))\n","vision_model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\n","vision_model.add(Conv2D(256, (3, 3), activation='relu'))\n","vision_model.add(MaxPooling2D((2, 2)))\n","vision_model.add(Flatten())\n","\n","image_input = Input(shape=(256, 256, 3))\n","encoded_image = vision_model(image_input)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UmGiOr7B1WG5"},"source":["# Define RNN for language input\n","question_input = Input(shape=[max_quest_length], dtype='int32')\n","embedded_question = Embedding(len(quest_wtoi)+1, output_dim=256, input_length=max_quest_length)(question_input)\n","encoded_question = LSTM(256)(embedded_question)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D1ZSy6sl1X-Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1611944375341,"user_tz":-60,"elapsed":543,"user":{"displayName":"Yeah Go","photoUrl":"","userId":"04798422462734009754"}},"outputId":"2e889d2f-bdbb-4b52-ecc5-6dd490c25af3"},"source":["# Combine CNN and RNN to create the final model\n","\n","merged = tf.keras.layers.concatenate([encoded_question, encoded_image])\n","output = Dense(58, activation='softmax')(merged)\n","vqa_model = Model(inputs=[image_input, question_input], outputs=output)\n","vqa_model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 22)]         0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 22, 256)      1188352     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 256)          525312      embedding[0][0]                  \n","__________________________________________________________________________________________________\n","sequential (Sequential)         (None, 215296)       1735488     input_1[0][0]                    \n","__________________________________________________________________________________________________\n","concatenate_1 (Concatenate)     (None, 215552)       0           lstm[0][0]                       \n","                                                                 sequential[0][0]                 \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 58)           12502074    concatenate_1[0][0]              \n","==================================================================================================\n","Total params: 15,951,226\n","Trainable params: 15,951,226\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AlbGCZQUT6Zp"},"source":["# Optimization params\n","# -------------------\n","\n","# Loss\n","loss = tf.keras.losses.CategoricalCrossentropy()\n","\n","# learning rate\n","lr = 1e-3\n","optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","# -------------------\n","\n","# Validation metrics\n","# ------------------\n","\n","metrics = ['accuracy']\n","# ------------------\n","\n","# Compile Model\n","vqa_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n","\n","import os\n","from datetime import datetime\n","\n","cwd = os.getcwd()\n","\n","exps_dir = os.path.join('/content/drive/My Drive/KerasRNN', 'translation_experiments')\n","if not os.path.exists(exps_dir):\n","    os.makedirs(exps_dir)\n","\n","now = datetime.now().strftime('%b%d_%H-%M-%S')\n","\n","exp_name = 'exp'\n","\n","exp_dir = os.path.join(exps_dir, exp_name + '_' + str(now))\n","if not os.path.exists(exp_dir):\n","    os.makedirs(exp_dir)\n","    \n","callbacks = []\n","\n","# Model checkpoint\n","# ----------------\n","ckpt_dir = os.path.join(exp_dir, 'ckpts')\n","if not os.path.exists(ckpt_dir):\n","    os.makedirs(ckpt_dir)\n","\n","ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(ckpt_dir, 'cp_{epoch:02d}.ckpt'), \n","                                                   save_weights_only=True)  # False to save the model directly\n","callbacks.append(ckpt_callback)\n","\n","# ----------------\n","\n","# Visualize Learning on Tensorboard\n","# ---------------------------------\n","tb_dir = os.path.join(exp_dir, 'tb_logs')\n","if not os.path.exists(tb_dir):\n","    os.makedirs(tb_dir)\n","    \n","# By default shows losses and metrics for both training and validation\n","tb_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_dir,\n","                                             profile_batch=0,\n","                                             histogram_freq=1)  # if 1 shows weights histograms\n","callbacks.append(tb_callback)\n","\n","# Early Stopping\n","# --------------\n","early_stop = True\n","if early_stop:\n","    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n","    callbacks.append(es_callback)\n","\n","# ---------------------------------\n","\n","\n","\n","# How to visualize Tensorboard\n","\n","# 1. tensorboard --logdir EXPERIMENTS_DIR --port PORT     <- from terminal\n","# 2. localhost:PORT   <- in your browser"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rINDnDXeq5WF"},"source":["# RUNN"]},{"cell_type":"code","metadata":{"id":"4qWqLTOIHvkA"},"source":["vqa_model.load_weights(\"/content/drive/My Drive/Weights/Custom_R_15Epochs.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXvh19UH9fLL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c81c7200-6829-46f8-e41d-e6f606c8c2c6"},"source":["vqa_model.fit(\r\n","  x=train_dataset,\r\n","  shuffle=True,\r\n","  epochs=3,\r\n","  steps_per_epoch=train_size/bs,\r\n","  validation_data=valid_dataset,\r\n","  validation_steps=valid_size/bs,\r\n","  callbacks=callbacks\r\n",")\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n"," 19/735 [..............................] - ETA: 2:01:54 - loss: 0.6219 - accuracy: 0.7490"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eyBMR32fTJfV"},"source":["import json\r\n","def read_questions_test(path):\r\n","      with open(path, 'r') as file:\r\n","        qs = json.load(file)\r\n","      \r\n","      image_ids=[]\r\n","      ids=[]\r\n","      for key,val in qs.items():  \r\n","        image_ids.append(val['image_id'])\r\n","        ids.append(key)        \r\n","      return (ids, image_ids)\r\n","\r\n","\r\n","ids_test, test_image_ids = read_questions_test(os.path.join(data_dir, 'test_questions.json'))                    \r\n","  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k4bPu2kzYNVG"},"source":["# Setup Test \r\n","from PIL import Image\r\n","def load_and_proccess_image(image_path):\r\n","    # Load image, then scale pixel values \r\n","    img = Image.open(image_path)\r\n","    img = img.resize([256,256])\r\n","    img_arr = np.array(img)/255.0\r\n","    \r\n","    return img_arr\r\n","\r\n","## Function which creates np.array given an index using test_image_ids\r\n","def load_img_from_index(index):\r\n","    id= str(test_image_ids[index]) + '.png'\r\n","    img_path = os.path.join(image_dir, id) \r\n","    img      =load_and_proccess_image(img_path)\r\n","    img      =np.array(img[:,:,1:4])\r\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ny27Jcaz9dk"},"source":["index=0\r\n","id=os.path.join(image_dir,str(test_image_ids[index]) + '.png')\r\n","img=load_and_proccess_image(id)\r\n","img.shape\r\n","img=np.array(img[:,:,1:4])\r\n","img.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rYMux4YyLXLl"},"source":["results = {}\r\n","index=0\r\n","\r\n","# load image using test_image_ids\r\n","img=load_img_from_index(index)\r\n","img=img[np.newaxis,:,:,:]\r\n","img.shape\r\n","q=quest_test_encoder_inputs[index]\r\n","q=q[np.newaxis,:]\r\n","q.shape\r\n","inputs={'input_1': img,'input_2':q}\r\n","out_sigmoid = vqa_model.predict(x=inputs)\r\n","out_sigmoid.shape\r\n","out_sigmoid\r\n","max(out_sigmoid[0,:])\r\n","predicted_class = tf.argmax(out_sigmoid[0,:])\r\n","predicted_class.shape\r\n","predicted_class\r\n","results[id]=predicted_class\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Xpg5ZvkbzQZ"},"source":["## Evaluate on test set\r\n","\r\n","import os\r\n","from datetime import datetime\r\n","\r\n","def create_csv(results, results_dir='./'):\r\n","\r\n","    csv_fname = 'results_'\r\n","    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\r\n","\r\n","    with open(os.path.join(results_dir, csv_fname), 'w') as f:\r\n","\r\n","        f.write('Id,Category\\n')\r\n","\r\n","        for key, value in results.items():\r\n","            f.write(key + ',' + str(value) + '\\n')\r\n","\r\n","results = {}\r\n","index=0\r\n","for id in ids_test:\r\n","  # load image using test_image_ids\r\n","  if (index%10==0):\r\n","    print(index)\r\n","  img=load_img_from_index(index)\r\n","  img=img[np.newaxis,:,:,:]\r\n","  q=quest_test_encoder_inputs[index]\r\n","  q=q[np.newaxis,:]\r\n","  inputs={'input_1': img,'input_2':q}\r\n","  out_sigmoid = vqa_model.predict(x=inputs)\r\n","  predicted_class = tf.argmax(out_sigmoid[0,:])\r\n","  results[id]=predicted_class.numpy()\r\n","  index+=1\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gw_iRYDr8-7P"},"source":["save_dir=r\"/content/drive/MyDrive\"\r\n","create_csv(results,save_dir)"],"execution_count":null,"outputs":[]}]}